diff --git a/llava/eval/run_llava.py b/llava/eval/run_llava.py
index 11bebda..003504a 100644
--- a/llava/eval/run_llava.py
+++ b/llava/eval/run_llava.py
@@ -9,6 +9,9 @@ from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, Keyw
 
 from PIL import Image
 
+import intel_extension_for_pytorch as ipex
+
+import os
 import requests
 from PIL import Image
 from io import BytesIO
@@ -56,9 +59,9 @@ def eval_model(args):
     prompt = conv.get_prompt()
 
     image = load_image(args.image_file)
-    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()
+    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].to(device='xpu', dtype=torch.bfloat16)
 
-    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
+    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to('xpu')
 
     stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
     keywords = [stop_str]
@@ -79,6 +82,7 @@ def eval_model(args):
     if n_diff_input_output > 0:
         print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
     outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
+
     outputs = outputs.strip()
     if outputs.endswith(stop_str):
         outputs = outputs[:-len(stop_str)]
diff --git a/llava/model/builder.py b/llava/model/builder.py
index eef47f0..44b6ef2 100644
--- a/llava/model/builder.py
+++ b/llava/model/builder.py
@@ -20,6 +20,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAn
 import torch
 from llava.model import *
 from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
+import intel_extension_for_pytorch as ipex
 
 
 def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map="auto"):
@@ -36,7 +37,11 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
             bnb_4bit_quant_type='nf4'
         )
     else:
-        kwargs['torch_dtype'] = torch.float16
+        kwargs['torch_dtype'] = torch.bfloat16
+
+    print("#### model_name:", model_name)
+    print("model_base: ", model_base)
+    print("#### kwargs:", kwargs)
 
     if 'llava' in model_name.lower():
         # Load LLaVA model
@@ -45,6 +50,7 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)
             print('Loading LLaVA from base model...')
             model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)
+            print("#### model.device:", model.device)
             token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features
             if model.lm_head.weight.shape[0] != token_num:
                 model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))
@@ -97,7 +103,8 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
                 model = LlavaMPTForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)
             else:
                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
-                model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)
+                model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True)
+                model = model.half().to(device='xpu', dtype=torch.bfloat16)
     else:
         # Load language model
         if model_base is not None:
@@ -134,7 +141,7 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
         vision_tower = model.get_vision_tower()
         if not vision_tower.is_loaded:
             vision_tower.load_model()
-        vision_tower.to(device='cuda', dtype=torch.float16)
+        vision_tower.to(device='xpu', dtype=torch.bfloat16)
         image_processor = vision_tower.image_processor
 
     if hasattr(model.config, "max_sequence_length"):
diff --git a/llava/serve/cli.py b/llava/serve/cli.py
index fbabbfa..5dc269e 100644
--- a/llava/serve/cli.py
+++ b/llava/serve/cli.py
@@ -14,6 +14,8 @@ from PIL import Image
 from io import BytesIO
 from transformers import TextStreamer
 
+import intel_extension_for_pytorch as ipex
+
 
 def load_image(image_file):
     if image_file.startswith('http') or image_file.startswith('https'):
@@ -52,7 +54,7 @@ def main(args):
         roles = conv.roles
 
     image = load_image(args.image_file)
-    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()
+    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().to(device='xpu', dtype=torch.bfloat16)
 
     while True:
         try:
@@ -79,7 +81,7 @@ def main(args):
         conv.append_message(conv.roles[1], None)
         prompt = conv.get_prompt()
 
-        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
+        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to('xpu')
         stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
         keywords = [stop_str]
         stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)
diff --git a/llava/serve/model_worker.py b/llava/serve/model_worker.py
index 4308fde..f3858a8 100644
--- a/llava/serve/model_worker.py
+++ b/llava/serve/model_worker.py
@@ -15,6 +15,8 @@ import torch
 import uvicorn
 from functools import partial
 
+import intel_extension_for_pytorch as ipex
+
 from llava.constants import WORKER_HEART_BEAT_INTERVAL
 from llava.utils import (build_logger, server_error_msg,
     pretty_print_semaphore)
@@ -33,7 +35,6 @@ global_counter = 0
 
 model_semaphore = None
 
-
 def heart_beat_worker(controller):
 
     while True:
@@ -135,9 +136,9 @@ class ModelWorker:
                 images = process_images(images, image_processor, model.config)
 
                 if type(images) is list:
-                    images = [image.to(self.model.device, dtype=torch.float16) for image in images]
+                    images = [image.to(self.model.device, dtype=torch.bfloat16) for image in images]
                 else:
-                    images = images.to(self.model.device, dtype=torch.float16)
+                    images = images.to(self.model.device, dtype=torch.bfloat16)
 
                 replace_token = DEFAULT_IMAGE_TOKEN
                 if getattr(self.model.config, 'mm_use_im_start_end', False):
@@ -159,7 +160,7 @@ class ModelWorker:
         stop_str = params.get("stop", None)
         do_sample = True if temperature > 0.001 else False
 
-        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
+        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to('xpu')
         keywords = [stop_str]
         stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)
         streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=15)
@@ -188,6 +189,7 @@ class ModelWorker:
             generated_text += new_text
             if generated_text.endswith(stop_str):
                 generated_text = generated_text[:-len(stop_str)]
+            print("#### generated_text:", generated_text)
             yield json.dumps({"text": generated_text, "error_code": 0}).encode() + b"\0"
 
     def generate_stream_gate(self, params):
