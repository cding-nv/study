# IPEX Source Code 

[Source](https://github.com/intel/intel-extension-for-pytorch)

## 1. å¯åŠ¨
./intel_extension_for_pytorch/xpu/lazy_init.py
           csrc/gpu/runtime Device.cpp

## 2. GPU Memory ç®¡ç†
intel-extension-for-pytorch/csrc/gpu/runtime
```
./csrc/runtime/CachingDeviceAllocator.cpp    
                 csrc/aten/core/Allocator.cpp          getDeviceAllocator()
./csrc/gpu/aten/tensor/OpaqueTensorFactories.cpp:15:  auto* allocator = xpu::dpcpp::getDeviceAllocator();
./csrc/gpu/aten/tensor/OpaqueTensorFactories.cpp:52:  auto* allocator = xpu::dpcpp::getDeviceAllocator();
./csrc/gpu/aten/operators/Set.cpp:50:      xpu::dpcpp::getDeviceAllocator(),
./csrc/gpu/aten/operators/TensorFactories.cpp:40:  auto* allocator = xpu::dpcpp::getDeviceAllocator();
./csrc/gpu/aten/quantized/QTensor.cpp:27:  at::Allocator* allocator = xpu::dpcpp::getDeviceAllocator();
```

## 3. model.to("xpu")
```
torch/nn/modules/module.py
  torch.no_grad()
  torch/autograd/grad_mode.py
```

## 4. torch.jit.trace()
```
    torch/jit/_trace.py  trace_module()
```

## 5. fusion_pass
csrc/gpu/jit/fusion_pass.cpp
```
pass_3()  -> xpu::FusionPass(g);  ->  OpFuser(graph->block(), graph).run();  --> processNode()
      
              ->isFusable() -> dnnlRules.find()
RegisterPreFusionPass::RegisterPreFusionPass(GraphPass p) {
  registerPrePass(std::move(p));    è¿™ä¸ªæ˜¯ torch pass_manager é‡Œçš„å‡½æ•° ï¼ï¼ï¼ï¼
}
```

## 6. ipex.optimize

## 7. ipex.optimize_transformers
```
transformers/optimize.py 482  optimize_transformersï¼ˆï¼‰
intel_extension_for_pytorch/frontend.py    optimization.fuse

        transformers/optimize.py 482  optimize_transformersï¼ˆï¼‰
intel_extension_for_pytorch/frontend.py    optimization.fuse

      ipex.optimize_transformers
è¿™ä¸ªæ˜¯  optimizer_transformers çš„Apply optimizations at Python frontend to the given transformers model (nn.Module).    This API focus on transformers models, especially for generation tasks inference.    Well supported model family: Llama, GPT-J, GPT-Neox, OPT, Falcon.

           ipex   frontend.py
                optimizer("O1")    
        # Apply optimizations at Python frontend to the given model (nn.Module), as
    well as the given optimizer (optional). If the optimizer is given,
    optimizations will be applied for training. Otherwise, optimization will be
    applied for inference. Optimizations include ``conv+bn`` folding (for
    inference only), weight prepacking and so on.
    import torch.fx.experimental.optimization as optimization
       optimization.fuse()
                        fuse conv+batchNorm
       linear_bn_fuse  : implementation follows https://github.com/pytorch/pytorch/blob/master/torch/fx/experimental/optimization.py#L50
             fuse linear + batchnorm
    utils._model_convert.replace_dropout_with_identity
    utils._model_convert.convert_module_data_type
    utils._weight_prepack.weight_prepack_with_ipex
    forward  torch.jit.trace
```
converter å®šä¹‰äº†ä¸€ç³»åˆ—è½¬æ¢çš„å‡½æ•°ï¼ŒåŒ…å«äº†module_replacer å’Œtensorslilcer 
module_replacerçš„ä½œç”¨å°±æ˜¯å› ä¸ºè¦ä½¿ç”¨æˆ‘ä»¬ipexå¯¹kernelçš„ä¼˜åŒ–ï¼Œæ‰€ä»¥å¿…é¡»æŠŠtransformerå®šä¹‰çš„moduleå’Œfunction hookè¿‡æ¥ï¼Œå®Œæˆæ›¿æ¢ï¼Œè¿™ä¸ªå®šä¹‰äº†æ›¿æ¢çš„æ“ä½œ
tensor silicer é¡¾åæ€ä¹‰ï¼Œå°±æ˜¯ä¸€ä¸ªtensor parallelçš„è¾…åŠ©å‡½æ•°
å“ªé‡Œ hook çš„ï¼Ÿdef default_replaced_module_dict():
```
    import transformers
    from diffusers.models.attention import BasicTransformerBlock
    default_replace_modules = {
        transformers.models.gptj.modeling_gptj.GPTJBlock: NewIPEXGPTJBlock,
        transformers.models.llama.modeling_llama.LlamaDecoderLayer: NewIPEXLLAMABlock,
        transformers.models.opt.modeling_opt.OPTDecoderLayer: NewIPEXOPTBlock,
        transformers.models.bloom.modeling_bloom.BloomBlock: NewIPEXBloomBlock,
        # only support transformers version model, not in-library model
        transformers.models.falcon.modeling_falcon.FalconDecoderLayer: NewIPEXFalconBlock,
        BasicTransformerBlock: NewIPEXBasicTransformerBlock,
    }
    return default_replace_modules
```

## 8. custom op å®ç°
```
IPEX_OP_REGISTER_DISPATCH(
      "beam_search_topk", beam_search_topk, c10::DispatchKey::XPU);
 
./csrc/utils/CustomOperatorRegistration.h
```

## 9. Attention SDPA
### Basic multi head attention
https://github.com/johnpzh/cudnn_samples_v8/blob/master/multiHeadAttention/attn_ref.py

### Flash attention
<img align="center" src="./flash_attention.png" width="100%" height="100%"> <BR>

1. https://fancyerii.github.io/2023/10/23/flashattention/    è®ºæ–‡è§£è¯»
2. M æ˜¯ SRAM çš„sizeã€‚ ğ‘ is the sequence length and ğ‘‘ is the head dimensionã€‚for GPT2, ğ‘ = 1024 and ğ‘‘ = 64    
3. ç¬¬12 è¡Œæ˜¯å…³é”®ï¼Œ  å±€éƒ¨çš„  Pij  ï¼ˆç›¸å½“äºæ±‚softmaxçš„åˆ†å­éƒ¨åˆ†ï¼‰å°±å¯ä»¥å’Œ V æå‰ç›¸ä¹˜äº†å¤šå°‘ä¸ª j ï¼Œ å°±è¦å’Œ V ä¹˜å¤šå°‘æ¬¡ï¼Œ è€ŒåŸå§‹çš„å®Œæ•´çš„ softmaxç»“æœå’Œ Vçš„ä¸€åˆ—åªä¹˜ä¸€æ¬¡ä¹˜çš„æ¬¡æ•°å˜å¤šï¼Œä½†æ¯æ¬¡ Pij çš„é•¿åº¦å˜çŸ­äº†    
4. å†…å¾ªç¯æ˜¯ Qi, ä¸€ä¸ªä¸€ä¸ªçš„ï¼Œ å»å’Œ  Kçš„æŸä¸€ä¸ªKj blockï¼Œ Vçš„æŸä¸€ä¸ª Vj blockï¼Œå»ä¹˜    
5. å¤–å¾ªç¯æ˜¯ K å’Œ V ç§»åˆ°ä¸‹ä¸€ä¸ª block    
6. flash attention æ˜æ˜¾å¯¹  batch/training å³å¤šquery æ˜¯æœ‰æ”¶ç›Šçš„ï¼Œé€šè¿‡å‡å°‘ hbm çš„è¯»å†™è®¿é—®ï¼Œé‚£ä¹ˆå¯¹ inference next token, æœ‰å¸®åŠ©å—ï¼Œ å³æ¯æ¬¡å°±ä¸€ä¸ª queryï¼Œï¼Ÿ    
7. ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œ ä¸€ä¸ª tokenï¼Œ å’Œå…ˆå‰æ‰€æœ‰token ï¼ˆåŒ…æ‹¬å®ƒè‡ªå·±ï¼‰ç®—attentionï¼Œ æŠŠè¿™ä¸ªtoken çš„æ‰€æœ‰ sotfmax éƒ½ç®—å‡ºæ¥å†ä¹˜Vï¼Œ SRAM åº”è¯¥æ˜¯æ”¾çš„ä¸‹çš„å§ã€‚ é™¤éå†å²token å¤ªå¤š ï¼ˆK V å¤ªå¤šï¼‰ï¼Œ SRAM è£…ä¸ä¸‹ï¼Œé‚£ä¹ˆç”¨ flash attention æ‰æœ‰æ”¶ç›Šå¯¹ä¹ˆï¼Ÿ   
8. Flash attention-2 ä¸»è¦å°±æ˜¯ æŠŠ query æ¨ªç€åˆ‡äº†ä¸€ä¸‹ ï¼Ÿ ä¹‹å‰ flash attention-1 æ˜¯ æ‰€æœ‰ token Qi ä¸€èµ·è®¡ç®—çš„  æ˜¯ä¹ˆï¼Ÿ    

### Flash decoding	
https://pytorch.org/blog/flash-decoding/    
flash-decoding åˆæŠŠ  Kï¼Œ V åˆ‡äº†å‡ åˆ€ï¼Œ ç›¸å½“äºå¤–å¾ªç¯å¹¶è¡Œèµ·æ¥äº†~~

https://crfm.stanford.edu/2023/10/12/flashdecoding.html    
https://github.com/facebookresearch/xformers/tree/main/examples/llama_inference    

### Code
```
csrc/gpu/aten/operators/transformers/attention.cpp
   xetla_fsdp_forward_atten_mask_alibi_strided()
   gpu::xetla::fmha_forward_kernel()

IPEX_LIBRARY_FRAGMENT() {
  IPEX_OP_REGISTER_DISPATCH(
      "xetla_fsdp_index_forward.xpu",
      at::AtenIpexTypeXPU::xetla_fsdp_index_forward,
      c10::DispatchKey::XPU);
}

csrc/gpu/aten/operators/comm/RegistrationDeclarations.h  _scaled_dot_product_efficient_attention
all_functions.yaml
xpu_functions.yaml

csrc/gpu/aten/operators/xetla/kernels/SDP/fmha_forward_causal_strided.cpp
   fmha_forward_kernel() 
         fmha_forward_causal_strided() -> fmha_forward_causal_strided_impl()
 
fmha_forward_causal_strided_impl() {
 
cgh.parallel_for(NdRange, [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
 fmha_fwd_op(ei, args);
}
fmha_forward_op_t = fmha_forward_causal_strided_t
fmha_forward_op_t fmha_fwd_op;
class fmha_forward_causal_strided_t {
   line 674:  inline KERNEL_FUNC void operator()
 
}
}

ipex.optimize_transformers()
    intel_extension_for_pytorch/transformers/optimize.py 
              optimize_transformers()
             model_convert_lowering()

    intel_extension_for_pytorch/transformers/models/xpu/optimize_transformers/modules/_transformers.py

intel_extension_for_pytorch/transformers/models/xpu/optimize_transformers/modules/_transformers.py
              def self_attention()
                  torch.xpu.IpexSDP
                  torch.xpu.IpexSDP_Index

csrc/gpu/aten/operators/xetla/kernels/SDP/fmha_forward.cpp
csrc/gpu/aten/operators/xetla/mha.h

libraries.gpu.xetla/include/common/utils/common.hpp  
         enum class mma_engine : uint8_t { xmx = 0, fpu = 1 };  
```

## 10. Segment KV cache
å¯¹ KV cache çš„ç†è§£    
https://zhuanlan.zhihu.com/p/410776234  attention è¾“å‡ºçš„å‘é‡å°±æ˜¯è¾“å…¥å‘é‡ç»è¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ æƒæ±‚å’Œä¹‹åçš„è¡¨ç¤ºã€‚
https://medium.com/@plienhar/llm-inference-series-2-the-two-phase-process-behind-llms-responses-1ff1ff021cd5   
Transformer è§£ç å™¨è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹å›¾ï¼Œ è§£ç å™¨è¾“å‡º logitsï¼Œç„¶åè¿›å…¥ decoding é˜¶æ®µï¼Œæ¯”å¦‚ greedyï¼Œ sampleï¼Œbeam search ç­‰ã€‚

<img align="center" src="./decoder_model.png" width="80%" height="80%"> <BR>

https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8       
å¦‚ä¸‹å›¾ï¼Œ     
1, è¾“å…¥ â€œWhat color is the sky? The sky â€ , å…¶ä¸­ï¼Œ â€œskyâ€œ æ˜¯æœ€åä¸€ä¸ª tokenï¼Œ output representations associated with this token were therefore produced using the representations from all the tokens in the sequence, i.e. the value vectors for â€œWhatâ€, â€œ colorâ€, â€œ isâ€, â€œ theâ€, â€œ skyâ€, â€œ?â€, â€œThe â€ and â€œsky â€ï¼Œ ç”Ÿæˆ â€œisâ€    
2. ä¸‹ä¸€æ­¥è¾“å…¥æ˜¯ â€œWhat color is the sky? The sky is â€œï¼Œ ä¸‹å›¾ æ·¡çº¢è‰²ï¼Œæ·¡ç´«è‰²ä»£è¡¨å†—ä½™è®¡ç®—çš„ keysï¼Œ values    
â€œisâ€ åŠ å…¥åˆ°è¾“å…¥ sequenceï¼Œ æˆ‘ä»¬éœ€è¦å“ªäº›ï¼š    
* A query vector for â€œis â€œ.    
* Key vectors for â€œWhatâ€, â€œ colorâ€, â€œ isâ€, â€œ theâ€, â€œ skyâ€, â€œ?â€, â€œThe â€ â€œsky â€ and â€œis â€ to compute attention scores.
* Value vectors for â€œWhatâ€, â€œ colorâ€, â€œ isâ€, â€œ theâ€, â€œ skyâ€, â€œ?â€, â€œThe â€ â€œsky â€ and â€œis â€ to compute the output.

å…¶ä¸­ â€œWhat color is the sky? The skyâ€ çš„ keysï¼Œ values åœ¨ä¹‹å‰å·²ç»è®¡ç®—è¿‡ï¼Œ åªéœ€è¦è®¡ç®— â€œisâ€ çš„ key å’Œ valueï¼Œ æˆ‘ä»¬åªéœ€è¦å“ªäº›ï¼š
* Computing a query, a key and a value for â€œis â€.
*	Fetching key and value vectors for â€œWhatâ€, â€œ colorâ€, â€œ isâ€, â€œ theâ€, â€œ skyâ€, â€œ?â€, â€œThe â€ and â€œsky â€ from the cache and concatenating them with the key and value we just computed for â€œis â€
* Computing the attention scores using the â€œis â€ query and all the keys.
* Computing the output vector for â€œis â€ using the attention scores and all the values.

<img align="center" src="./kv_cache.png" width="100%" height="100%"><BR>

## 11. Beam search
<img align="center" src="./beam_search.png" width="80%" height="80%"><BR>

## 12. torch.compile
[../torch.compile](../torch.compile/) 

 
