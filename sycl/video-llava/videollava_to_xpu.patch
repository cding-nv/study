diff --git a/videollava/model/builder.py b/videollava/model/builder.py
index ac6ac95..b66f06e 100644
--- a/videollava/model/builder.py
+++ b/videollava/model/builder.py
@@ -41,7 +41,7 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
             bnb_4bit_quant_type='nf4'
         )
     else:
-        kwargs['torch_dtype'] = torch.float16
+        kwargs['torch_dtype'] = torch.bfloat16
 
     if 'llava' in model_name.lower():
         # Load LLaVA model
@@ -145,7 +145,7 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
             image_tower = model.get_image_tower()
             if not image_tower.is_loaded:
                 image_tower.load_model()
-            image_tower.to(device=device, dtype=torch.float16)
+            image_tower.to(device=device, dtype=torch.bfloat16)
             image_processor = image_tower.image_processor
             processor['image'] = image_processor
 
@@ -153,7 +153,7 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
             video_tower = model.get_video_tower()
             if not video_tower.is_loaded:
                 video_tower.load_model()
-            video_tower.to(device=device, dtype=torch.float16)
+            video_tower.to(device=device, dtype=torch.bfloat16)
             video_processor = video_tower.video_processor
             processor['video'] = video_processor
     # ==========================================================================================================
@@ -162,5 +162,4 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
         context_len = model.config.max_sequence_length
     else:
         context_len = 2048
-
     return tokenizer, model, processor, context_len
diff --git a/videollava/serve/cli.py b/videollava/serve/cli.py
index 95aa6d7..5e6a474 100644
--- a/videollava/serve/cli.py
+++ b/videollava/serve/cli.py
@@ -2,6 +2,7 @@ import argparse
 import os
 
 import torch
+import intel_extension_for_pytorch as ipex
 
 from videollava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, \
     DEFAULT_VIDEO_TOKEN
@@ -30,6 +31,11 @@ def main(args):
     tokenizer, model, processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name,
                                                                      args.load_8bit, args.load_4bit,
                                                                      device=args.device, cache_dir=args.cache_dir)
+    # To xpu and IPEX optimize
+    model = model.half().to(device=args.device, dtype=torch.bfloat16)
+    model.eval()
+    model = ipex.optimize(model, dtype=torch.bfloat16)
+
     image_processor, video_processor = processor['image'], processor['video']
     if 'llama-2' in model_name.lower():
         conv_mode = "llava_llama_2"
@@ -54,12 +60,13 @@ def main(args):
     tensor = []
     special_token = []
     args.file = args.file if isinstance(args.file, list) else [args.file]
+
     for file in args.file:
         if os.path.splitext(file)[-1].lower() in image_ext:
-            file = image_processor.preprocess(file, return_tensors='pt')['pixel_values'][0].to(model.device, dtype=torch.float16)
+            file = image_processor.preprocess(file, return_tensors='pt')['pixel_values'][0].to(model.device, dtype=torch.bfloat16)
             special_token += [DEFAULT_IMAGE_TOKEN]
         elif os.path.splitext(file)[-1].lower() in video_ext:
-            file = video_processor(file, return_tensors='pt')['pixel_values'][0].to(model.device, dtype=torch.float16)
+            file = video_processor(file, return_tensors='pt')['pixel_values'][0].to(model.device, dtype=torch.bfloat16)
             special_token += [DEFAULT_IMAGE_TOKEN] * model.get_video_tower().config.num_frames
         else:
             raise ValueError(f'Support video of {video_ext} and image of {image_ext}, but found {os.path.splitext(file)[-1].lower()}')
