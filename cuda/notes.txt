1. 使得需要线程间通信的计算尽可能在单个线程块中执行    
2.  In a SIMT architecture, rather than a single thread issuing vector instructions applied to data vectors, multiple threads issue common instructions to arbitrary data. SIMD 是单线程issue 向量指令作用于 data， SIMT 是多线程    

3. 在 Kepler（SM 3.0）及之后架构中，引入专用硬件模块处理 shfl：
   允许每个线程在 shfl 执行阶段，以另一个 lane ID 为索引读取其寄存器值
   这类似于一个 跨寄存器 lane-select MUX
   并且只适用于同一个 warp，因为 warp 同时调度、无时序不确定性   
    val = RegFile[warp_id][thread_id = 0][val_reg_index]
   不能跨 warp 实现？因为：
     跨 warp 之间没有同步执行保障（divergent）
     不同 warp 执行可能在 pipeline 的不同 stage
     寄存器文件划分也有局部性，跨 warp 的 lane 定址会带来极大复杂性与延迟
    因此，__shfl_* 只能用于 warp 内通信，warp 之间仍需 shared memory + barrier 同步

    https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/


ZLUDA
Cuda 转译的限制   EULA  (最终用户许可协议 (End-user license agreement)
Cuda 编译出来的 binary 即机器码， 可以翻译成 intel gpu 或者 AMD gpu 对应的指令， 翻译过程是 一边翻译一边执行，翻译好的不要再翻译，所以越跑越快
ZLUDA 用的就是这个办法 https://github.com/vosen/ZLUDA
指令集是公开的 或者可以通过反编译手段从 binary 获取 ( binary 的文件格式是固定的， 比如 elf 格式等)

AI Bot
把 AI Bot 当作应用程序，把大模型当作操作系统。
 1. 一开始只能用提示词来编写 Bot 
2. 后来系统开放了插件/知识库/数据库，使 Bot 多了一些自定义能力
 3. 后来又开放了工作流，多 Agent ，使开发者可以用编程的逻辑来自定义 Bot
信念-期望-意图（Belief-Desire-lntension， BDI) 体系架构是混合型体系架构的一个重要类型。Agent的表示形式，Agent的行为可以被描述成好像拥有信念、期望和意图等思维状态。信念表示Agent拥有的知识，期望描述Agent追求的目标，意图说明Agent选择计划以实现哪些目标
一个好的平台产品，如果能够高效地让更多的工程师像操作标准化软件一样进行模型的微调（finetuning）、部署以及再训练的闭环操作，将会开拓一个非常大的市场 
在一个孤立系统里，如果没有外力做功，其总混乱度（即熵）会不断增大，这就是熵增定律
一个企业如果不能持续创新和进步，很快就会被淘汰，最终走向灭亡

《少有的人走的路》里面如此解释自律：
因为所有的事物都在向着无规律、无序和混乱的方向发展，如果你要变得自律，你就得逆着熵增做功，这个过程会非常痛苦。
熵减就是建立秩序，其关键是获取能量，并对之有效利用，建立适合目的的功能结构。


smooth quant
把 activation 的异常值scale 加载到 weight 上smooth 之后再量化
GPTQ
分析权重中哪些权重对最终的精度损失影响是最大的，  有校验数据集 可以从网上下载
Hessian 矩阵 研究每一个相互影响，改进成行与行的相互影响
量化没一列 对其他列有影响，lazy batch update，是分 group， 累计 quant error， 加到一块再update
AWQ
根据输入来找 outlier
Outlier保持 fp16， 其他的用 int4， 类似 LLM.int8
进一步用 类似 smoothquant 的方法
最终结果好不好取决于 校验数据集找的好不好


K8S 解决的是管理，调度，workload编排的问题，有可能可以提高效率，可能不直接和性能有关
SRIOV是通过PCI的虚拟化虚出VGPU.硬件虚拟化
 虚拟化有很多软件实现方式 比如从 kernel， 从 userspace 也可以， 但 mig 是从硬件上实现的 隔离
K8S是个集群管理系统，构建几十，成百服务器的大规模集群系统时使用。当然如果想用一台服务器也可以用，只是没必要而已

https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf

 Nemotron-4 340B 模型系列，包括 Nemotron-4-340B-Base、Nemotron-4-340B-Instruct 和 Nemotron-4-340B-Reward。我们的模型根据 NVIDIA 开放模型许可协议开放访问，该许可协议是一种宽松的模型许可，允许分发、修改和使用模型及其输出。这些模型在广泛的评估基准上的表现与开放访问模型相媲美，并且在以 FP8 精度部署时，其大小适合在具有 8 个 GPU 的单个 DGX H100 上使用。我们相信社区可以在各种研究和商业应用中从这些模型中受益，尤其是用于生成合成数据以训练较小的语言模型。值得注意的是，我们的模型对齐过程中使用的 98% 以上的数据都是合成生成的，展示了这些模型在生成合成数据方面的有效性。为了进一步支持开放研究并促进模型开发，我们还开源了模型对齐过程中使用的合成数据生成管道。

医疗，药物研发，金融，制造业，零售业


大型语言模型 (LLM) 在不同应用中的许多任务上都非常有效。最近的努力集中在通过对更多、更高质量的 token 进行预训练来提高这些模型的准确性。例如，Llama-2 系列 (Touvron 等人，2023) 是在 2 万亿个 token 上进行训练的，而 Llama-3 系列 (MetaAI，2024) 是在 15 万亿个 token 上进行训练的。Nemotron-4 340B 基础模型使用来自高质量数据集的 9 万亿个 token 进行训练，详细信息请见 Parmar 等人 (2024)。
我们将基础 LLM 与监督微调 (SFT) 对齐，然后进行偏好微调，例如带人类反馈的强化学习 (RLHF)（Ouyang 等人，2022 年；Bai 等人，2022 年）和直接偏好优化 (DPO)（Rafailov 等人，2024 年）。对齐过程使模型能够更好地遵循指令、有效地参与对话并更好地解决问题。对齐过程依赖于可以准确识别响应质量的奖​​励模型。该奖励模型是 RLHF 中的关键组成部分，也是合成数据生成中质量过滤和偏好排名的有用工具。
为了支持整个社区持续开发 LLM，我们推出了 Nemotron-4-340B-Base、Nemotron-4-340B-Instruct 和 Nemotron-4-340B-Reward，它们作为具有宽松许可证的开放访问模型发布。图 1 突出显示了 Nemotron-4 340B 模型系列在选定任务中的准确性。具体来说，我们表明 Nemotron-4-340B-Base 在常识推理任务（如 ARC-Challenge、MMLU 和 BigBench Hard 基准）上与 Llama-3 70B（MetaAI，2024）、Mixtral 8x22B（Mistral-AI-Team，2024b）和最近发布的 Qwen-2 72B 模型等开放访问基础模型具有竞争力。 Nemotron-4-340B-Instruct 在指令遵循和聊天功能方面超越了相应的指令模型（MetaAI，2024；Mistral-AI-Team，2024b；Qwen-Team，2024）。截至发布时，Nemotron-4-340B-Reward 在 RewardBench（Allen AI，2024）上实现了最高准确率，甚至超越了 GPT-4o-0513 和 Gemini 1.5 Pro-0514 等专有模型。我们发布奖励模型是为了支持社区中 LLM 的持续发展。
这些模型的一个有前途的应用是合成数据生成，这已经在提高预训练数据质量方面显示出巨大的价值。例如，数据合成已用于重新表述网络文本（Maini 等人，2024 年）、为文本质量分类器生成训练数据（MetaAI，2024 年；Guilherme Penedo，2024 年）以及为预训练集中代表性不足的领域创建数据。此外，由于收集人工注释数据的成本很高，因此合成数据生成对于对齐至关重要。我们大量使用合成数据来创建 Nemotron-4-340B-Instruct：在我们的对齐过程中，超过 98% 的训练数据都是合成生成的。除了分享我们的模型和对齐策略外，我们还发布了我们的synthetic data generation pipeline，其中包括合成提示生成、响应和对话生成、质量过滤和偏好排名。该流程旨在支持supervised fine-tuning and preference fine-tuning,，我们相信它有可能通过创建可适应广泛领域的高质量数据来造福社区。
通过发布 Nemotron-4-340B-Base、Nemotron-4-340B-Instruct 和 Nemotron-4-340B-Reward 并分享我们的合成数据生成流程，我们希望鼓励广泛使用大型、功能强大的模型，以加速 AI 应用程序开发和 LLM 负责任使用的研究进展。我们致力于负责任的开发实践，不打算将该模型用于生成有毒或有害内容。

贡献摘要：
• 我们根据 NVIDIA 开放模型许可协议发布了 Nemotron-4 340B 模型系列，包括 Nemotron-4-340B-Base、Nemotron-4-340B-Instruct 和 Nemotron-4-340B-Reward，该协议允许商业应用。
• 我们发布了用于训练和推理这些模型的代码，以提高透明度和可重复性。
• 我们提供有关synthetic data generation pipeline的全面详细信息，并说明其在model alignment方面的有效性。我们还分享了我们的生成提示、我们的人工注释偏好数据集以及用于质量过滤和偏好排名preference ranking的 Nemotron-4-340B-Reward。展望未来，我们将分享更多工具，例如用于合成数据生成的 NVIDIA Inference Microservices (NIMs)。
2.1 data
我们的预训练数据混合由三种不同类型的数据组成：英语自然语言数据（70%）、多语言自然语言数据（15%）和源代码数据（15%）。英语语料库由来自各种来源和领域的精选文档组成，包括网络文档、新闻文章、科学论文、书籍等。我们的多语言数据包含 53 种自然语言，由单语和平行语料库的文档组成，而我们的代码数据集由 43 种编程语言组成。我们在此数据上训练总共 9T 个标记，前 8T 作为正式预训练阶段进行，最后 1T 作为持续预训练阶段进行 （ formal pretraining phase and the last 1T in a continued pretraining phase）。有关我们的训练语料库和策展程序的更详细细分，我们参考了 Parmar 等人 (2024)，因为 Nemotron-4-340B-Base 遵循与 Nemotron-4-15B-Base 相同的数据混合。

2.2 Arch
Nemotron-4-340B-Base 的架构与 Nemotron-4-15B-Base 相似（Parmar 等人，2024 年）。它是一种标准的仅解码器 Transformer 架构（Vaswani 等人，2017 年），具有因果注意力掩码causal attention masks,，使用旋转位置嵌入 (RoPE)（Su 等人，2021 年）、SentencePiece tokenizer（Kudo 和 Richardson，2018 年）和 MLP 层中的平方 ReLU 激活。它no bias，dropout 率为零，并且untied input-output embeddings。我们还使用grouped query attention (GQA)（Ainslie 等人，2023 年）。Nemotron-4-340B-Base 的超参数如表 1 所示。它有 94 亿个嵌入参数和 3316 亿个非嵌入参数

2.3 training detail
Nemotron-4-340B-Base 使用 768 个 DGX H100 节点进行训练；每个节点包含 8 个基于 NVIDIA Hopper 架构的 H100 80GB SXM5 GPU（NVIDIA，2022 年）。在执行无稀疏性的 16 位浮点 (bfloat16) 算法时，每个 H100 GPU 的峰值吞吐量为 989 teraFLOP/s。在每个节点内，GPU 通过 NVLink 和 NVSwitch (nvl,) 连接；GPU 到 GPU 的带宽为 900 GB/s（每个方向 450 GB/s）。每个节点都有 8 个 NVIDIA Mellanox 400 Gbps HDR InfiniBand 主机通道适配器 (HCA) 用于节点间通信。我们结合使用了 8 路张量并行 (Shoeybi 等人，2019 年)、12 路交错流水线并行 (Narayanan 等人，2021 年) 和数据并行来训练模型；我们还使用分布式优化器将优化器状态分片到数据并行副本上，并减少训练的内存占用。随着批处理大小的增加，数据并行度从 16 扩展到 64。表 2 总结了批处理大小增加的 3 个阶段，包括每次迭代时间和模型 FLOP/s 利用率 (MFU) (Chowdhery 等人，2022 年；Korthikanti 等人，2022 年)。MFU 量化了 GPU 在模型训练中的利用效率，其中 100% 是理论峰值。

Tensor parallel:  8
Pipeline parallel: 96 layers / 8layers = 12                      data parallel:    (768x8) / (8x12)=64
2.4 Pertaining  -> Continued training
我们发现，在模型训练结束时切换数据分布和学习率衰减计划可显著提高模型质量。具体来说，在对 8T 个 token 进行预训练后，我们使用相同的损失目标并对 1T 个额外的 token 进行继续训练。在继续训练的这个额外阶段，我们使用两个不同的数据分布。第一个分布构成了继续训练 token 的大多数，并使用了在预训练期间已经引入的 token，但其分布将更大的采样权重放在更高质量的源上。第二个分布引入了少量的问答式对齐示例，以便模型更好地在下游评估中回答这些问题，同时还增加来自模型准确度较低的领域的数据源的权重。结合优先考虑更陡的衰减斜率而不是学习率幅度的学习率计划，我们发现这种数据分布的排序和样式允许模型从预训练数据集平稳过渡，并更好地从训练最后阶段引入的数据中学习。

Nemotron-4-340B-Reward 的总体得分很高，这证明了我们的 Nemotron-4-340B-Base 模型的强大、HelpSteer2 数据集的高质量以及我们方法的有效性。此外，该奖励模型为训练 Nemotron-4-340B-Instruct 提供了坚实的基础，我们将在后续章节中讨论这一点。

3.2 Alignment Data 
随着模型的不断改进，我们发现现有的宽松数据集越来越不足以训练最对齐的模型。此外，从人类那里收集高质量的数据是一项耗时且昂贵的工作。为了应对这一挑战，我们对synthetic data generation (SDG) 进行了深入探索，将其作为解决方案。值得注意的是，在整个对齐过程中，我们仅依赖大约 20K 人工注释数据（10K 用于监督微调，10K Helpsteer2 数据用于奖励模型训练和偏好微调），而我们的数据生成管道合成了用于监督微调和偏好微调的 98% 以上的数据。在本节中，我们将详细描述我们的synthetic data generation pipeline，以及它与其他人工数据的集成。
3.2.1  Prompt Preparation
尽管已有可用的提示，例如 LMSYS-Chat-1M 提示（Zheng 等人，2023），但生成合成提示是 SDG 中重要的第一步。这种方法使我们能够控制提示分布以覆盖多种场景。提示多样性是多维的 - 它涉及任务多样性（例如，写作、开放式问答、封闭式问答）、主题多样性（例如，STEM、人文、日常生活）和指令多样性（例如，json 输出、# 段落、是或否答案）。为了确保这些维度的提示多样性，我们采用了与 UltraChat 数据集（Ding 等人，2023）和 CAMEL（Li 等人，2023）生成类似的方法。具体来说，我们使用permissive的 Mixtral-8x7B-Instruct-v0.1（Jiang 等，2024）作为生成器，分别为开放式问答、写作、封闭式问答、数学和编码等任务生成合成提示。对于每个提示任务，我们用一组不同的主题或关键字为生成提供种子，以便提示涵盖各种各样的主题。我们还会生成遵循提示的指令，这些提示明确定义预期响应的格式，例如“输出必须采用 json 格式。”。此外，我们生成包含用户助手交互历史记录的两轮提示，以提高我们模型的对话技巧。我们将在以下段落中讨论生成单轮合成提示、遵循指令的提示和两轮提示的流程。
Synthetic single-turn prompts.我们在图 2 中展示了生成合成提示的高级流程。为了收集不同的主题，我们提示生成器输出一组不同的宏观主题。然后，我们提示生成器输出每个合成宏观主题的相关子主题。包括合成宏观主题、合成子主题和手动收集的主题，我们总共收集了 3K 个主题。我们通过提示生成器生成与每个给定主题相关的问题来生成合成的开放式问答提示（例如，“什么是机器学习？”）。然后，要求生成器将问题细化得更详细和具体，因为我们观察到最初生成的问题通常很短。对于与写作相关的提示（例如，“写一篇关于机器学习的文章。”），提示包括有关生成给定主题的某些类型文档（例如，时事通讯、Ding 等人（2023）中的论文）的说明。同样，我们要求生成器细化生成的任务以包含更多细节。我们使用 C4 数据集 (Raffel et al., 2020) 中的文本来生成封闭式问答提示。对于每个给定的文档，我们要求生成器输出相应的指令（例如“总结给定的文本”或“根据给定的文本，xxx 是什么？”）。然后，我们使用手动定义的模板将文档与生成的指令连接起来。为了生成数学和编码提示，我们从数学和 Python 编程中收集了一组不同的关键字（例如，除法、循环、lambda 函数）。然后，我们为数学和 Python 编程生成高级主题和子主题。接下来，我们提示生成器分别对维基百科实体是否与数学或 Python 编程相关进行分类。我们还解析我们的 Python 预训练数据以收集频繁的 Python 关键字并包括手动收集的与数学相关的关键字。总体而言，我们收集了 12K 个与 Python 相关的关键字和 17K 个与数学相关的关键字。然后我们提示生成器生成与每个关键字相关的问题。在补充材料 B 中，我们分享了在这些管道中用于合成提示生成的提示。


Synthetic instruction-following prompts.
Instruction-following对于对齐模型至关重要。为了提高我们模型的指令遵循能力，我们生成合成指令遵循提示，例如“写一篇关于机器学习的文章。你的回答应该有三个段落。”。具体来说，我们选择一组随机的合成提示。对于每个合成提示，我们从 Zhou et al. (2023) 中的“可验证”指令模板中随机生成一个合成指令（例如，“你的回答应该有三个段落。”）。然后，我们将提示和指令与手动定义的模板连接在一起。除了单轮指令遵循提示之外，我们还构建了多轮指令遵循提示，其中指令适用于所有未来的对话，例如，“根据以下内容回答问题和所有后续问题：[指令开始] 用三个段落回答。[指令结束]”。我们还构建了第二轮指令遵循提示，要求根据给定的指令修改之前的响应。
Synthetic two-turn prompts.
虽然监督微调阶段的对话数据集通常是多轮的，但偏好微调的偏好数据通常是单轮的（Bai 等人，2022；Cui 等人，2023）。为了提高模型在偏好微调中的多轮对话技能，我们构建了两轮提示来构建偏好数据集。具体来说，提示包含一个用户问题、一个助手答案和另一个用户问题，形式为“用户：XXX；助手：XXX；用户：XXX；”。我们从 ShareGPT（RyokoAI，2023）获取第一个用户提示，并使用我们的中间指导模型生成助手响应和下一轮问题。
Real-world LMSYS prompts.
为了更好地反映真实世界的用户请求，我们还从 LMSYS-Chat-1M (LMSYS) (Zheng 等人，2023) 中提取提示。我们以平衡的比例组合所有提示，并将它们分成两个不同的集合，一个用于监督学习，另一个用于偏好学习，确保两者之间没有重叠。在监督学习拆分中，我们还会从 LMSYS 中删除标记为潜在不安全的提示，以避免引发不必要的对话。但是，我们保留了偏好学习拆分中的那些，让模型学会区分安全和不安全的响应。在图 3 中，我们展示了合成单轮提示和 LMSYS 提示之间的比较。具体来说，对于每组提示，我们使用 Mixtral-8x7B-Instruct-v0.1 模型生成响应，并使用 Nemotron-4-340B-Reward 注释响应的有用性分数。我们绘制了合成提示和 LMSYS 提示的有用性分布。我们观察到合成提示的平均有用性高于 LMSYS 提示。由于简单提示更容易“有用”，这意味着 LMSYS 提示平均比合成单轮提示更难、更复杂。
3.2.2Synthetic Dialogue Generation
监督微调使模型能够学习如何以对话形式与用户交互。我们通过提示指示模型根据输入提示生成响应来启动合成对话。为了培养多轮对话能力，我们将每个对话设计为包含三个回合，从而创建更具动态和互动性的对话流程。通过迭代角色扮演，模型交替模拟助手和用户的角色。为了在用户回合中引出所需的行为，我们发现必须为模型提供明确的提示，这些提示定义了不同的用户个性（如补充材料 C 中所述），并附上对话历史。我们还通过排除礼貌的语句（例如“谢谢……”，“当然我很乐意……”）对用户回合进行后处理，以模仿现实世界的用户问题。采用贪婪抽样进行演示数据合成。此外，我们利用 Nemotron-4-340B-Reward 来评估对话的质量，为每个样本分配一个分数，并过滤掉低于预定阈值的样本。这提供了额外的质量控制层，确保只保留高质量的数据。
3.2.3  Synthetic Preference Data Generation合成偏好数据生成
我们使用 10K 人工注释的 HelpSteer2 偏好数据来训练 Nemotron-4-340B-Reward，但我们还需要具有更多样化提示领域的偏好数据、来自我们顶级中间模型的更高质量的响应以及在可用时具有额外的地面实况信号。因此，我们努力以三元组形式（提示、选择的响应、拒绝的响应）生成合成偏好数据。
Response generation.响应生成。
偏好数据包含合成的单轮提示、指令遵循提示、两轮提示以及真实世界的提示，包括 ShareGPT 提示、LMSYS 提示以及来自 GSM8K（Cobbe 等人，2021 年）和 MATH（Hendrycks 等人，2021 年）训练数据集的提示。对于每个提示，我们使用多个随机中间模型生成响应。利用多个模型生成响应可确保偏好数据集具有多样化的响应供模型学习。此外，当响应是根据 MT-Bench 从我们表现最佳的模型中随机生成的多个响应时，我们还构建了更具挑战性的合成偏好示例。这些具有挑战性的偏好示例使我们的模型能够进一步改进自身。
Ground-Truth-as-a-Judge.以事实为依据作为判断。
给定每个提示的多个响应，我们需要判断它们的偏好排名并选择所选和拒绝的响应。有些任务可以使用真实标签（例如，GSM8K 和 MATH 训练数据集中的答案）或验证器（例如，可以使用 Python 程序验证响应后的指令）进行评估，我们使用真实标签/验证器来判断每个响应的正确性。我们选择正确的响应作为所选响应，将不正确的响应作为拒绝的响应。
LLM-as-Judge 和 Reward-Model-as-Judge。
大多数提示都没有客观的答案。我们尝试了 LLM-as-Judge 和 Reward-Model-as-Judge。在 LLM-as-Judge 中，我们向评判 LLM 提供提示和两个响应，并要求其比较这两个响应。为了避免位置偏差，我们以交换的响应顺序两次询问 LLM。当 LLM 在两次中都有一致的评判者时，我们会选择一个有效的（提示、所选、拒绝）三元组。评判提示在补充材料 D 中。虽然 LLM-as-Judge 为我们早期的偏好数据集迭代提供支持，但我们进一步探索了 Reward-Model-as-Judge，我们要求 Nemotron-4-340B-Reward 预测每个（提示、响应）对的奖励并根据奖励决定偏好排名。Reward Bench 分数（Lambert 等人，2024）表明，Reward-Model-as-Judge 的准确度高于 LLM-as-Judge。具体来说，在 Chat-Hard 类别中，选择的和拒绝的响应很难区分，Reward-Model-as-Judge 的表现远远好于 LLM-as-Judge，平均准确度为 0.87 vs 0.54。我们注意到，Chat-Hard 类别分数对于合成数据生成中的偏好排名特别重要。因此，我们在以后的数据集迭代中改用 Reward-Model-as-Judge。

3.2.4  Iterative Weak-to-Strong Alignment
如前所述，高质量数据对于模型对齐至关重要。在数据合成中，对齐的 LLM 需要在整个生成流程中准确遵循指令。这提出了重要的问题：哪种模型最适合用作生成器；生成器强度与数据质量有何关系；以及我们如何改进数据生成器。受弱到强泛化（Burns 等人，2023 年）的启发，我们开发了一种新颖的迭代方法来逐步完善我们的数据以达到最优。这种方法结合了对齐训练和数据合成的优势，使它们能够相互增强并推动持续改进。
图 4 说明了迭代弱到强对齐的工作流程。（学生胜过老师）在这里，模型的质量（无论被认为是弱的还是强的）由多个评估指标的组合定义（有关基础模型，请参见第 2.4 节，有关指导模型，请参见第 3.4.1 节），而与模型大小无关。初始对齐模型用作对话和偏好数据的生成器。然后，使用监督微调和偏好调整将数据用于对齐更好的基础模型。有趣的是，我们发现教师模型不会对学生模型施加上限。具体而言，随着基础模型和对齐数据的完善，新对齐的模型能够显著超越初始对齐模型。请注意，对齐过程与基础模型预训练并行执行。在第一次迭代中，我们选择 Mixtral-8x7B-Instruct-v0.1 作为初始对齐模型，因为它已被证明是具有宽松许可的强模型。生成的数据用于训练 Nemotron-4-340B-Base 的中间检查点，称为 340B-Interm-1-Base。值得注意的是，340B-Interm-1-Base 的表现优于 Mixtral 8x7B Base 模型，这反过来又使生成的 340B-Interm-1-Instruct 模型超越了 Mixtral-8x7B-Instruct-v0.1 模型。这反映了我们可以在弱监督下获得强大能力的事实。在第二次迭代中，我们利用生成的 340B-Interm-1-Instruct 模型作为新的数据生成器。鉴于其与 Mixtral-8x7B-Instruct-v0.1 相比能力增强，第二次迭代中生成的合成数据比第一次迭代中生成的数据质量更高。生成的数据用于训练 340B-Interm-2-Base 成为 340B-Interm-2-Chat。这种迭代过程产生了一种自我强化的飞轮效应，其改进可以归因于两个方面：（1）当使用相同的数据集时，基础模型的强度对指导模型有直接影响，基础模型越强，指导模型就越强；（2）相反，当使用相同的基础模型时，数据集的质量在决定指导模型的有效性方面起着至关重要的作用，更高质量的数据可以产生更强大的指导模型。在整个对齐过程中，我们进行多轮数据生成和改进，不断提高模型的质量。

3.2.5 其他数据源
我们整合了几个补充数据集，以赋予模型特定功能，如下所示。
Topic following。主题连贯性和细粒度指令跟踪是指令模型的重要功能。我们整合了 CantTalkAboutThis（Sreedhar 等人，2024 年）的训练集，其中包括涵盖广泛主题的合成对话，故意穿插干扰性转折，以将聊天机器人从主要主题转移开。此数据集有助于增强模型在面向任务的交互过程中专注于预期主题的能力。
Incapable tasks.。由于需要特定功能（例如互联网访问或实时知识），模型可能无法自行完成某些任务。为了减轻这些情况下的幻觉，我们采用了少量方法，使用人类编写的示例（参见补充材料 A）来提示 LLM 生成各种各样的问题。然后，我们明确要求 LLM 以拒绝的方式做出回应，收集这些回应并将它们与相应的问题配对。这些配对数据用于训练我们的模型，使其能够更好地处理它无法完成的任务。
STEM 数据集。Open-Platypus (Lee 等人，2023) 已被证明可以提高 STEM 和逻辑知识。我们将具有宽松许可证的子集（PRM800K（Lightman 等人，2023 年）、SciBench（Wang 等人，2023a 年）、ARB（Sawada 等人，2023 年）、openbookQA（Mihaylov 等人，2018 年））纳入我们的训练数据中。
Document-based reasoning and QA.基于文档的问答是 LLM 的一个重要用例。我们利用 FinQA 数据集 (Chen et al., 2021b) 来提高数值推理能力，使用来自 (Liu et al., 2024) 的人工注释数据来提高情境化问答的准确性，并使用 wikitablequestions 数据集 (Pasupat and Liang, 2015) 来加强模型对半结构化数据的理解
3.3 对齐算法 Alignment Algorithms
我们采用标准协议 (Ouyang et al., 2022) 进行模型对齐，涉及两个阶段：监督微调和偏好微调。在本节中，我们将详细介绍底层算法并介绍我们的创新训练策略。
3.3.1 Staged Supervised Fine-tuning分阶段监督微调
监督微调 (SFT) 是对齐的第一步。传统上，SFT 在一个阶段执行，其中数据集包含来自所有任务的样本混合。然而，我们的实验结果表明，同时学习多种行为有时会导致它们之间发生冲突，从而阻止模型同时在所有任务上实现最佳对齐。我们在编码任务中特别强烈地观察到这种现象，其中调整数据混合的采样权重无法使模型与所有编码任务对齐。为了解决这个问题，我们设计了一个两阶段的 SFT 策略，使模型能够以连续和慎重的方式获取不同的行为。我们发现这种方法在所有下游任务中都能产生优异的效果。
Code SFT.
为了在不干扰其他任务的情况下提高编码和推理能力，我们在第一阶段仅对编码数据进行 SFT。我们发现，需要大量数据才能有效提高模型的编码能力。为了有效地合成编码数据，我们开发了 Genetic Instruct，这是一种模仿进化过程的方法，利用自我指令（Wang 等人，2022 年）和向导编码器突变（Luo 等人，2023 年）从有限数量的高质量种子中创建大量合成样本。在这种方法中，我们还引入了一个适应度函数，它使用 LLM 来评估生成的指令及其解决方案的正确性和质量。通过这些评估和检查的样本被添加到种群池中，进化过程持续进行，直到达到目标种群规模。整个管道设计用于与多个种群群落高效并行执行，从而可以根据需要进行扩展。经过大量去重和过滤后，约 800K 个样本的精选数据集被保留用于 Code SFT 训练。我们使用 3e-7 的恒定学习率和 128 的全局批处理大小对模型进行一个周期的训练。
General SFT.通用 SFT。在第二阶段，我们继续进行通用 SFT，利用包含各种任务的 200K 样本的混合数据集，如第 3.2 节所述。为了降低遗忘风险，数据混合还包括来自前一个代码 SFT 阶段的 2% 的代码生成样本。我们使用全局批量大小 128 对模型进行三个时期的训练，并在 [1e-7, 5e-7] 范围内进行 LR 搜索。对于这两个阶段，我们都屏蔽了用户轮次，只计算助手轮次的损失。

3.3.2Preference Fine-tuning 
偏好微调在监督微调阶段之后，我们继续通过偏好微调来改进模型，其中我们的模型以（提示、选择的响应、拒绝的响应）三元组的形式学习偏好示例（Ouyang 等人，2022 年；Bai 等人，2022 年）。具体来说，我们的偏好微调阶段涉及模型改进的多次迭代，同时使用直接偏好优化（Rafailov 等人，2024）和我们的新对齐算法，即奖励感知偏好优化。
Direct Preference Optimization (DPO).
DPO（Rafailov 等人，2024）算法优化策略网络，以最大化所选和拒绝的响应之间的隐式奖励差距。虽然策略学会区分所选和拒绝的响应，但我们观察到所选和拒绝的响应的可能性都会随着差距的增加而持续下降，即使所选响应质量很高。从经验上讲，我们观察到策略网络在训练时间足够长时往往会过度拟合，并且一个指标（例如 MT-Bench）的改进通常伴随着其他指标（例如 0-shot MMLU）的下降。我们尝试通过在原始 DPO 损失之外对所选响应添加加权 SFT 损失来缓解这些问题。额外的 SFT 损失有助于防止策略网络偏离偏好数据太多，特别是因为我们的偏好数据不是从参考策略生成的。为了避免模型学习低质量的选择响应，我们使用 Nemotron-4-340B-Reward 在无法获得真实数据时挑选具有高质量选择响应的示例。这会产生一个包含各种任务的 160K 示例的偏好数据集。我们训练模型一个时期，全局批量大小为 256，学习率恒定。我们将学习率调整在 [3e-8, 3e-7] 范围内，DPO 损失中的 kl 正则化系数调整在 [3e-4, 3e-3] 范围内，SFT 损失的权重调整在 [1e-5, 1e-3] 范围内。 
Reward-aware Preference Optimization (RPO).如第 3.2.3 节所述，我们的大多数偏好数据都是合成的，其偏好等级是根据 Nemotron-4-340B-Reward 的奖励来判断的。虽然 DPO 仅使用两个响应之间的二进制顺序，但奖励之间的差异包含更多信息。从经验上讲，我们观察到一些被拒绝的响应仅比所选响应稍差，而一些被拒绝的响应则远远落后。由于不了解质量差距，DPO 努力最大化所选响应和被拒绝响应的隐性奖励差距，这会导致过度拟合和不必要地“忘记”高质量的被拒绝响应。为了解决这个问题，我们提出了一种新算法，即奖励感知偏好优化 (RPO)，它尝试使用策略网络定义的隐性奖励来近似奖励差距。具体来说，这会导致一个新的损失函数，


3.4Instruct 模型评估
3.4.1 自动基准测试我们在广泛的自动基准测试中对 Nemotron-4-340B-Instruct 进行了全面评估。在本节中，我们报告了模型的结果，并与开源（Llama-3-70B-Instruct (MetaAI, 2024)、Mixtral-8x22B-Instruct-v0.1 (Mistral-AI-Team, 2024b)、Qwen-2-72B-Instruct (Qwen-Team, 2024) 和专有（GPT-4-1106-preview (OpenAI, 2023)、Mistral Large (Mistral-AI-Team, 2024a)、Claude-3-Sonnet (Anthropic, 2024)）对齐模型进行了比较。
如表 5 所示，Nemotron-4-340B-Instruct 与目前可用的开放获取模型具有竞争力。对于 instruct 模型，我们认为零样本评估是最重要的设置，因为它评估了模型在没有先前示例的情况下准确遵循指令的能力。此设置更接近人们在现实世界中与 LLM 互动的方式。为了提高透明度和可重复性，我们在补充材料 E 1 中包含了用于评估的提示。如第 3.3 节所述，我们的对齐训练涉及多个阶段：Code SFT、General SFT、DPO 和三轮 RPO。我们测量最终模型的结果，并在表 6 中量化对齐的每个阶段中每个中间模型的强度。我们观察到 CodeSFT 阶段将 HumanEval 从基础模型的 57.3 显着提高到 70.7。接下来的 General SFT 大大提高了其他类别（如 MT-Bench 和 MMLU）的准确性，而 HumanEval 略有下降。 DPO 步骤进一步提高了大多数指标，而 MT-bench 略有下降。最后，RPO 步骤统一提升了所有指标。具体来说，MT-Bench 从 7.90 增加到 8.22，IFEval Prompt-Strict-Acc 从 61.7 增加到 79.9。
3.4.2 人工评估
除了自动评估外，我们还使用专门的训练有素的注释者团队对我们的模型进行了人工评估。这些注释者被呈现了 136 个提示，分为 10 个不同的任务类别，并使用 6 点李克特量表评估响应。该量表包括五个质量级别和一个额外的级别，用于模型完全无法遵循指令的情况。提示类别主要来自 InstructGPT（Ouyang 等人，2022 年），并增加了一个多轮聊天类别，其中只评估了最后一个助手轮次。杂项“其他”类别包括有关纯推理和对抗性提示的提示。提示的详细分布包含在补充材料 G 中。我们的注释指南有两个主要方面：有用性和真实性。基于这些轴，我们详细说明了 5 个质量等级中每个等级应主要包含的内容，因为与通常的“差”/“优秀”极端值相比，它往往通过减少主观性（Joshi 等人，2015）来提供更好的可靠性。在指南的迭代改进过程中，我们发现，通过加入次要终点来考虑注释者对响应长度的看法，可以改善结果。这种方法有助于将个人的冗长偏好与模型遵循说明和提供有用答案的能力区分开来。
3.4.3 安全评估
随着 LLM 的普及，与其使用相关的内容安全风险也在增加。为了评估我们模型的安全性，我们采用了 AEGIS（Ghosh 等人，2024），这是 NVIDIA 的高质量内容安全解决方案和评估基准。AEGIS 由广泛的内容安全风险分类法支持，该分类法涵盖了人与 LLM 交互中的 12 个关键风险（详情请参阅补充材料 H）。该分类法是通过考虑多个内容安全风险分类法中最相关的社区风险而创建的。它符合 NVIDIA 对仇恨和骚扰类别下受保护特征的组织价值观，并将对未成年人的性虐待定义为单独的关键危险类别。我们还引入了一个新类别“需要谨慎”，以解决没有足够背景来确定安全性的模糊情况。此类别在更倾向于采用防御模式而非宽松模式的场景中特别有用，因为“需要谨慎”可以根据需要映射到不安全或安全。作为基准，AEGIS 包含一个人工注释的用户提示、单轮和多轮对话数据集，以及 AEGIS 安全模型，该模型可以预测候选 LLM 的响应是安全还是不安全，并在响应不安全时提供违规类别。AEGIS 安全模型是一组基于开源 LlamaGuard（Inan 等人，2023 年）LLM 的分类器，这些分类器以参数高效的方式使用 AEGIS 安全分类法和策略进行了进一步指令调整。


结论
我们展示了 Nemotron-4 340B 系列模型：Nemotron-4-340B-Base、Nemotron-4-340B-Instruct 和 Nemotron-4-340B-Reward。它们在permissive的开放获取许可下提供，我们详细介绍了它们在广泛任务中的能力。我们发布了这些模型的训练和推理代码。我们还提供了有关我们的synthetic data generation pipeline的全面详细信息，并说明了其有效性。我们相信这些模型将促进 LLM 和 AI 应用程序的进一步发展。



